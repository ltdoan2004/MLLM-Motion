{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%cd /home/ltdoanh/jupyter/jupyter/ldtan/NExT-GPT/code\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python3 train.py "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python3 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python3 /home/ltdoanh/jupyter/jupyter/ldtan/NExT-GPT/code/model/MotionDiffuse/tools/evaluation.py /home/ltdoanh/jupyter/jupyter/ldtan/MotionDiffuse/t2m/t2m_new_ver4/opt.txt 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doanh = ['a person walks upward to the right side and then walks backward to the left side', 'a person moving something from right to left', 'a person reaches up and then down with his right hand.', 'the person has joking back-and-forth from the right to the left.', 'a man bend at his hips and touches his right knee then touches his left knee.', 'person walks forward slowly, turns around 180 degrees anti-clockwise, and then sprints forwards', 'the person is walking forward.', 'a person walks forward, curving to their right while doing so', 'the person slowly walks forward and then stops and stands.', 'a person stands with arms and legs spread apart and jumps in place repeatedly while holding the rest of their body still.', 'the person in the video is walking left to right.', 'a person who bowed down in a greeting manner', 'a person stands with elbows bent and hands in front of the body, and rolls arms slowly to their left and back to the center again, dropping in the right arm.', 'a man is going out for a jog.', 'a man walks forward while balancing himself and then walks to the left.', 'a person lifts their upturned hands to shoulder height in a shrug, then stretches their arms straight out so they are parallel to the ground.', 'a person jogs back and forth once then walks back to the start', 'a person runs right, fakes a move, then runs left.', 'a man walks forward 2 steps then back to his original place.', 'prson is shaking hands in front of themselves.', \"a person runs, using short strides, first counterclockwise from 9 o'clock to 12 o'clock, then forward, and finally clockwise from 6 o'clock to 10 o'clock.\", 'this person lifts his left arm then lowers it.', 'a person walks to the left and stops.', 'this person jogs quickly forward.', 'a person uses their left hand to point', 'person stands still lifts right hand to mouth and does small circular motions', 'a person kicks something with their right leg.', 'a figure lifts something from waist height on the right and sets it down shoulder height on the left', 'a person walks backwards in a counter counterclockwise motion.', 'a person clapping their hands', 'person was walking towards the left.', 'a person does 2 high jumps.']\n",
    "len(doanh)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(doanh)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%cd /home/ltdoanh/jupyter/jupyter/ldtan/NExT-GPT/code/model/MotionDiffuse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%cd /home/ltdoanh/jupyter/jupyter/ldtan/MotionDiffuse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python -u /home/ltdoanh/jupyter/jupyter/ldtan/MotionDiffuse/text2motion/tools/train.py  --name moe_ckpt --batch_size 32 --times 50  --dataset_name t2m --checkpoints_dir /home/ltdoanh/jupyter/jupyter/ldtan/MotionDiffuse\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------ Options -------------\n",
      "batch_size: 1\n",
      "checkpoints_dir: ./checkpoints\n",
      "data_parallel: False\n",
      "dataset_name: t2m\n",
      "decomp_name: Decomp_SP001_SM001_H512\n",
      "diffusion_steps: 1000\n",
      "dim_att_vec: 512\n",
      "dim_dec_hidden: 1024\n",
      "dim_movement_dec_hidden: 512\n",
      "dim_movement_enc_hidden: 512\n",
      "dim_movement_latent: 512\n",
      "dim_pos_hidden: 1024\n",
      "dim_pri_hidden: 1024\n",
      "dim_text_hidden: 512\n",
      "dim_z: 128\n",
      "distributed: False\n",
      "est_length: False\n",
      "estimator_mod: bigru\n",
      "ext: default\n",
      "gpu_id: -1\n",
      "is_train: False\n",
      "latent_dim: 512\n",
      "max_text_len: 20\n",
      "motion_length: 0\n",
      "n_layers_dec: 1\n",
      "n_layers_pos: 1\n",
      "n_layers_pri: 1\n",
      "name: test\n",
      "no_clip: False\n",
      "no_eff: False\n",
      "num_layers: 8\n",
      "num_results: 40\n",
      "repeat_times: 3\n",
      "result_path: /home/ltdoanh/jupyter/jupyter/ldtan/MotionDiffuse/results\n",
      "split_file: test.txt\n",
      "start_mov_len: 10\n",
      "text: \n",
      "text_enc_mod: bigru\n",
      "text_file: \n",
      "unit_length: 4\n",
      "which_epoch: latest\n",
      "-------------- End ----------------\n",
      "Reading /home/ltdoanh/jupyter/jupyter/ldtan/MotionDiffuse/t2m/t2m_new_ver2/opt.txt\n",
      "Reading /home/ltdoanh/jupyter/jupyter/ldtan/MotionDiffuse/t2m/t2m_new_ver2/opt.txt\n",
      "Loading dataset t2m ...\n",
      "100%|█████████████████████████████████████| 4384/4384 [00:00<00:00, 5404.79it/s]\n",
      "Pointer Pointing at 0\n",
      "Ground Truth Dataset Loading Completed!!!\n",
      "doanh\n",
      "torch.Size([8, 77, 512])\n",
      "torch.Size([8, 2048])\n",
      "torch.Size([8, 77, 256])\n",
      "8\n",
      "100%|███████████████████████████████████████| 1000/1000 [00:17<00:00, 57.47it/s]\n",
      "torch.Size([8, 196, 263])\n",
      "doanh\n",
      "torch.Size([8, 77, 512])\n",
      "torch.Size([8, 2048])\n",
      "torch.Size([8, 77, 256])\n",
      "8\n",
      "100%|███████████████████████████████████████| 1000/1000 [00:17<00:00, 58.45it/s]\n",
      "torch.Size([8, 196, 263])\n",
      "doanh\n",
      "torch.Size([8, 77, 512])\n",
      "torch.Size([8, 2048])\n",
      "torch.Size([8, 77, 256])\n",
      "8\n",
      "100%|███████████████████████████████████████| 1000/1000 [00:18<00:00, 52.74it/s]\n",
      "torch.Size([8, 196, 263])\n",
      "doanh\n",
      "torch.Size([8, 77, 512])\n",
      "torch.Size([8, 2048])\n",
      "torch.Size([8, 77, 256])\n",
      "8\n",
      " 13%|█████▎                                  | 132/1000 [00:02<00:15, 54.94it/s]^C\n",
      " 13%|█████▎                                  | 133/1000 [00:02<00:15, 54.86it/s]\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/ltdoanh/jupyter/jupyter/ldtan/MotionDiffuse/text2motion/tools/evaluation.py\", line 444, in <module>\n",
      "    mae , velocity_error, jerk_error, pae = score(trainer, gt_dataset, dim =3)\n",
      "  File \"/home/ltdoanh/jupyter/jupyter/ldtan/MotionDiffuse/text2motion/tools/evaluation.py\", line 91, in score\n",
      "    predicted = trainers.generate(all_caption, all_m_lens, opt.dim_pose)\n",
      "  File \"/home/ltdoanh/jupyter/jupyter/ldtan/MotionDiffuse/text2motion/trainers/ddpm_trainer.py\", line 124, in generate\n",
      "    output = self.generate_batch(batch_caption, batch_m_lens, dim_pose)\n",
      "  File \"/home/ltdoanh/jupyter/jupyter/ldtan/MotionDiffuse/text2motion/trainers/ddpm_trainer.py\", line 100, in generate_batch\n",
      "    output = self.diffusion.p_sample_loop(\n",
      "  File \"/home/ltdoanh/jupyter/jupyter/ldtan/MotionDiffuse/text2motion/models/gaussian_diffusion.py\", line 702, in p_sample_loop\n",
      "    for sample in self.p_sample_loop_progressive(\n",
      "  File \"/home/ltdoanh/jupyter/jupyter/ldtan/MotionDiffuse/text2motion/models/gaussian_diffusion.py\", line 757, in p_sample_loop_progressive\n",
      "    out = self.p_sample(\n",
      "  File \"/home/ltdoanh/jupyter/jupyter/ldtan/MotionDiffuse/text2motion/models/gaussian_diffusion.py\", line 649, in p_sample\n",
      "    out = self.p_mean_variance(\n",
      "  File \"/home/ltdoanh/jupyter/jupyter/ldtan/MotionDiffuse/text2motion/models/gaussian_diffusion.py\", line 471, in p_mean_variance\n",
      "    model_output = model(x, self._scale_timesteps(t), **model_kwargs)\n",
      "  File \"/home/ltdoanh/.local/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1501, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/home/ltdoanh/jupyter/jupyter/ldtan/MotionDiffuse/text2motion/models/transformer.py\", line 438, in forward\n",
      "    src_mask = self.generate_src_mask(T, length).to(x.device).unsqueeze(-1)\n",
      "  File \"/home/ltdoanh/jupyter/jupyter/ldtan/MotionDiffuse/text2motion/models/transformer.py\", line 419, in generate_src_mask\n",
      "    src_mask[i, j] = 0\n",
      "KeyboardInterrupt\n"
     ]
    }
   ],
   "source": [
    "!python -u /home/ltdoanh/jupyter/jupyter/ldtan/MotionDiffuse/text2motion/tools/evaluation.py --result_path /home/ltdoanh/jupyter/jupyter/ldtan/MotionDiffuse/results\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/ltdoanh/jupyter/jupyter/ldtan/MotionDiffuse/text2motion\n"
     ]
    }
   ],
   "source": [
    "%cd /home/ltdoanh/jupyter/jupyter/ldtan/MotionDiffuse/text2motion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading /home/ltdoanh/jupyter/jupyter/ldtan/MotionDiffuse/t2m/t2m_new_ver2/opt.txt\n",
      "/home/ltdoanh/.local/lib/python3.10/site-packages/torch/nn/modules/transformer.py:307: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)\n",
      "  warnings.warn(f\"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}\")\n",
      "/home/ltdoanh/jupyter/jupyter/ldtan/MotionDiffuse/text2motion/trainers/ddpm_trainer.py:173: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint = torch.load(model_dir, map_location=self.device)\n",
      "doanh\n",
      "torch.Size([1, 77, 512])\n",
      "100%|███████████████████████████████████████| 1000/1000 [00:18<00:00, 53.90it/s]\n",
      "torch.Size([1, 120, 263])\n",
      "(120, 263)\n",
      "(120, 22, 3)\n"
     ]
    }
   ],
   "source": [
    "!python -u tools/visualization.py \\\n",
    "    --opt_path /home/ltdoanh/jupyter/jupyter/ldtan/MotionDiffuse/t2m/t2m_new_ver2/opt.txt \\\n",
    "    --text \"he does a big long jump\" \\\n",
    "    --motion_length 120 \\\n",
    "    --result_path \"/home/ltdoanh/jupyter/jupyter/ldtan/NExT-GPT/test_sample_doanh_123.gif\" \\\n",
    "    --gpu_id 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved frame 0 to /home/ltdoanh/jupyter/jupyter/ldtan/NExT-GPT/test/img/frame_000.png\n",
      "Saved frame 1 to /home/ltdoanh/jupyter/jupyter/ldtan/NExT-GPT/test/img/frame_001.png\n",
      "Saved frame 2 to /home/ltdoanh/jupyter/jupyter/ldtan/NExT-GPT/test/img/frame_002.png\n",
      "Saved frame 3 to /home/ltdoanh/jupyter/jupyter/ldtan/NExT-GPT/test/img/frame_003.png\n",
      "Saved frame 4 to /home/ltdoanh/jupyter/jupyter/ldtan/NExT-GPT/test/img/frame_004.png\n",
      "Saved frame 5 to /home/ltdoanh/jupyter/jupyter/ldtan/NExT-GPT/test/img/frame_005.png\n",
      "Saved frame 6 to /home/ltdoanh/jupyter/jupyter/ldtan/NExT-GPT/test/img/frame_006.png\n",
      "Saved frame 7 to /home/ltdoanh/jupyter/jupyter/ldtan/NExT-GPT/test/img/frame_007.png\n",
      "Saved frame 8 to /home/ltdoanh/jupyter/jupyter/ldtan/NExT-GPT/test/img/frame_008.png\n",
      "Saved frame 9 to /home/ltdoanh/jupyter/jupyter/ldtan/NExT-GPT/test/img/frame_009.png\n",
      "Saved frame 10 to /home/ltdoanh/jupyter/jupyter/ldtan/NExT-GPT/test/img/frame_010.png\n",
      "Saved frame 11 to /home/ltdoanh/jupyter/jupyter/ldtan/NExT-GPT/test/img/frame_011.png\n",
      "Saved frame 12 to /home/ltdoanh/jupyter/jupyter/ldtan/NExT-GPT/test/img/frame_012.png\n",
      "Saved frame 13 to /home/ltdoanh/jupyter/jupyter/ldtan/NExT-GPT/test/img/frame_013.png\n",
      "Saved frame 14 to /home/ltdoanh/jupyter/jupyter/ldtan/NExT-GPT/test/img/frame_014.png\n",
      "Saved frame 15 to /home/ltdoanh/jupyter/jupyter/ldtan/NExT-GPT/test/img/frame_015.png\n",
      "Saved frame 16 to /home/ltdoanh/jupyter/jupyter/ldtan/NExT-GPT/test/img/frame_016.png\n",
      "Saved frame 17 to /home/ltdoanh/jupyter/jupyter/ldtan/NExT-GPT/test/img/frame_017.png\n",
      "Saved frame 18 to /home/ltdoanh/jupyter/jupyter/ldtan/NExT-GPT/test/img/frame_018.png\n",
      "Saved frame 19 to /home/ltdoanh/jupyter/jupyter/ldtan/NExT-GPT/test/img/frame_019.png\n",
      "Saved frame 20 to /home/ltdoanh/jupyter/jupyter/ldtan/NExT-GPT/test/img/frame_020.png\n",
      "Saved frame 21 to /home/ltdoanh/jupyter/jupyter/ldtan/NExT-GPT/test/img/frame_021.png\n",
      "Saved frame 22 to /home/ltdoanh/jupyter/jupyter/ldtan/NExT-GPT/test/img/frame_022.png\n",
      "Saved frame 23 to /home/ltdoanh/jupyter/jupyter/ldtan/NExT-GPT/test/img/frame_023.png\n",
      "Saved frame 24 to /home/ltdoanh/jupyter/jupyter/ldtan/NExT-GPT/test/img/frame_024.png\n",
      "Saved frame 25 to /home/ltdoanh/jupyter/jupyter/ldtan/NExT-GPT/test/img/frame_025.png\n",
      "Saved frame 26 to /home/ltdoanh/jupyter/jupyter/ldtan/NExT-GPT/test/img/frame_026.png\n",
      "Saved frame 27 to /home/ltdoanh/jupyter/jupyter/ldtan/NExT-GPT/test/img/frame_027.png\n",
      "Saved frame 28 to /home/ltdoanh/jupyter/jupyter/ldtan/NExT-GPT/test/img/frame_028.png\n",
      "Saved frame 29 to /home/ltdoanh/jupyter/jupyter/ldtan/NExT-GPT/test/img/frame_029.png\n",
      "Saved frame 30 to /home/ltdoanh/jupyter/jupyter/ldtan/NExT-GPT/test/img/frame_030.png\n",
      "Saved frame 31 to /home/ltdoanh/jupyter/jupyter/ldtan/NExT-GPT/test/img/frame_031.png\n",
      "Saved frame 32 to /home/ltdoanh/jupyter/jupyter/ldtan/NExT-GPT/test/img/frame_032.png\n",
      "Saved frame 33 to /home/ltdoanh/jupyter/jupyter/ldtan/NExT-GPT/test/img/frame_033.png\n",
      "Saved frame 34 to /home/ltdoanh/jupyter/jupyter/ldtan/NExT-GPT/test/img/frame_034.png\n",
      "Saved frame 35 to /home/ltdoanh/jupyter/jupyter/ldtan/NExT-GPT/test/img/frame_035.png\n",
      "Saved frame 36 to /home/ltdoanh/jupyter/jupyter/ldtan/NExT-GPT/test/img/frame_036.png\n",
      "Saved frame 37 to /home/ltdoanh/jupyter/jupyter/ldtan/NExT-GPT/test/img/frame_037.png\n",
      "Saved frame 38 to /home/ltdoanh/jupyter/jupyter/ldtan/NExT-GPT/test/img/frame_038.png\n",
      "Saved frame 39 to /home/ltdoanh/jupyter/jupyter/ldtan/NExT-GPT/test/img/frame_039.png\n",
      "Saved frame 40 to /home/ltdoanh/jupyter/jupyter/ldtan/NExT-GPT/test/img/frame_040.png\n",
      "Saved frame 41 to /home/ltdoanh/jupyter/jupyter/ldtan/NExT-GPT/test/img/frame_041.png\n",
      "Saved frame 42 to /home/ltdoanh/jupyter/jupyter/ldtan/NExT-GPT/test/img/frame_042.png\n",
      "Saved frame 43 to /home/ltdoanh/jupyter/jupyter/ldtan/NExT-GPT/test/img/frame_043.png\n",
      "Saved frame 44 to /home/ltdoanh/jupyter/jupyter/ldtan/NExT-GPT/test/img/frame_044.png\n",
      "Saved frame 45 to /home/ltdoanh/jupyter/jupyter/ldtan/NExT-GPT/test/img/frame_045.png\n",
      "Saved frame 46 to /home/ltdoanh/jupyter/jupyter/ldtan/NExT-GPT/test/img/frame_046.png\n",
      "Saved frame 47 to /home/ltdoanh/jupyter/jupyter/ldtan/NExT-GPT/test/img/frame_047.png\n",
      "Saved frame 48 to /home/ltdoanh/jupyter/jupyter/ldtan/NExT-GPT/test/img/frame_048.png\n",
      "Saved frame 49 to /home/ltdoanh/jupyter/jupyter/ldtan/NExT-GPT/test/img/frame_049.png\n",
      "Saved frame 50 to /home/ltdoanh/jupyter/jupyter/ldtan/NExT-GPT/test/img/frame_050.png\n",
      "Saved frame 51 to /home/ltdoanh/jupyter/jupyter/ldtan/NExT-GPT/test/img/frame_051.png\n",
      "Saved frame 52 to /home/ltdoanh/jupyter/jupyter/ldtan/NExT-GPT/test/img/frame_052.png\n",
      "Saved frame 53 to /home/ltdoanh/jupyter/jupyter/ldtan/NExT-GPT/test/img/frame_053.png\n",
      "Saved frame 54 to /home/ltdoanh/jupyter/jupyter/ldtan/NExT-GPT/test/img/frame_054.png\n",
      "Saved frame 55 to /home/ltdoanh/jupyter/jupyter/ldtan/NExT-GPT/test/img/frame_055.png\n",
      "Saved frame 56 to /home/ltdoanh/jupyter/jupyter/ldtan/NExT-GPT/test/img/frame_056.png\n",
      "Saved frame 57 to /home/ltdoanh/jupyter/jupyter/ldtan/NExT-GPT/test/img/frame_057.png\n",
      "Saved frame 58 to /home/ltdoanh/jupyter/jupyter/ldtan/NExT-GPT/test/img/frame_058.png\n",
      "Saved frame 59 to /home/ltdoanh/jupyter/jupyter/ldtan/NExT-GPT/test/img/frame_059.png\n",
      "All frames are saved in /home/ltdoanh/jupyter/jupyter/ldtan/NExT-GPT/test/img.\n"
     ]
    }
   ],
   "source": [
    "from PIL import Image\n",
    "import os\n",
    "\n",
    "def split_gif_to_images(gif_path, output_folder):\n",
    "    try:\n",
    "        # Open the GIF file\n",
    "        gif = Image.open(gif_path)\n",
    "        \n",
    "        # Create the output folder if it doesn't exist\n",
    "        if not os.path.exists(output_folder):\n",
    "            os.makedirs(output_folder)\n",
    "\n",
    "        frame_number = 0\n",
    "        while True:\n",
    "            # Save each frame as an image\n",
    "            frame_path = os.path.join(output_folder, f\"frame_{frame_number:03d}.png\")\n",
    "            gif.save(frame_path)\n",
    "            print(f\"Saved frame {frame_number} to {frame_path}\")\n",
    "            frame_number += 1\n",
    "            \n",
    "            try:\n",
    "                # Move to the next frame\n",
    "                gif.seek(gif.tell() + 1)\n",
    "            except EOFError:\n",
    "                # Break when no more frames are left\n",
    "                break\n",
    "        \n",
    "        print(f\"All frames are saved in {output_folder}.\")\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred: {e}\")\n",
    "\n",
    "# Usage\n",
    "gif_path = \"/home/ltdoanh/jupyter/jupyter/ldtan/NExT-GPT/test_sample.gif\"  # Replace with the path to your GIF file\n",
    "output_folder = \"/home/ltdoanh/jupyter/jupyter/ldtan/NExT-GPT/test/img\"    # Replace with the desired output folder\n",
    "split_gif_to_images(gif_path, output_folder)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%cd /home/ltdoanh/jupyter/jupyter/ldtan/motion_data/body_model/\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!tar -xvJf /home/ltdoanh/jupyter/jupyter/ldtan/motion_data/body_model/smplh.tar.xz\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "def count_files(directory):\n",
    "    total_files = 0\n",
    "    for root, dirs, files in os.walk(directory):\n",
    "        total_files += len(files)\n",
    "    return total_files\n",
    "\n",
    "# Example usage\n",
    "directory_path = '/home/ltdoanh/jupyter/jupyter/ldtan/KIT-ML/new_joint_vecs'\n",
    "\n",
    "\n",
    "file_count = count_files(directory_path)\n",
    "print(f'There are {file_count} files in {directory_path}')\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# Đường dẫn tới thư mục chứa các file .txt.npy\n",
    "folder_path = '/home/ltdoanh/jupyter/jupyter/ldtan/NExT-GPT/data/T-X_pair_data/motion/embed'\n",
    "\n",
    "# Duyệt qua tất cả các file trong thư mục\n",
    "for filename in os.listdir(folder_path):\n",
    "    # Kiểm tra nếu file có đuôi .txt.npy\n",
    "    if filename.endswith('.txt.npy'):\n",
    "        # Tạo tên mới bằng cách bỏ phần .txt\n",
    "        new_filename = filename.replace('.txt.npy', '.npy')\n",
    "        \n",
    "        # Đường dẫn đầy đủ của file cũ và file mới\n",
    "        old_file = os.path.join(folder_path, filename)\n",
    "        new_file = os.path.join(folder_path, new_filename)\n",
    "        \n",
    "        # Đổi tên file\n",
    "        os.rename(old_file, new_file)\n",
    "        print(f\"Đã đổi tên {filename} thành {new_filename}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nextgpt",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
